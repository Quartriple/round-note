import { useState, useRef, useCallback, useEffect } from 'react';

// WebSocket ì„¤ì • ê´€ë ¨ ì „ì—­ ìƒìˆ˜
const WS_URL = process.env.REACT_APP_API_URL ? 
    `ws${process.env.REACT_APP_API_URL.substring(4)}/api/v1/realtime/ws` : 
    'ws://localhost:8000/api/v1/realtime/ws';

// ì˜¤ë””ì˜¤ ì„¤ì • (Deepgram ìš”êµ¬ì‚¬í•­ì— ë§ì¶¤)
const AUDIO_CONFIG = {
    sampleRate: 16000,
    channel: 1, // Web Audio APIëŠ” ëª¨ë…¸(1) ì±„ë„ë§Œ ì²˜ë¦¬
    bufferSize: 4096,
};

const useRealtimeStream = () => {
    // 1. ìƒíƒœ ì •ì˜
    const [isRecording, setIsRecording] = useState(false);
    const [transcript, setTranscript] = useState('');
    const [partialText, setPartialText] = useState('');
    const [translation, setTranslation] = useState('');

    // 2. Mutable ê°ì²´ ì°¸ì¡° (ë¦¬ì•¡íŠ¸ ë Œë”ë§ ì—†ì´ ê°’ì„ ìœ ì§€)
    const wsRef = useRef(null); // WebSocket ì—°ê²° ê°ì²´
    const mediaStreamRef = useRef(null); // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ê°ì²´
    const audioProcessorRef = useRef(null); // ì˜¤ë””ì˜¤ ë…¸ë“œ(Node) ê°ì²´
    const audioContextRef = useRef(null); // AudioContext ê°ì²´


    // ì´ í•¨ìˆ˜ëŠ” ëª¨ë“  ë¦¬ì†ŒìŠ¤(WebSocket, ë§ˆì´í¬, ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ)ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    // stopRecordingê³¼ startRecordingì˜ catch ë¸”ë¡ì—ì„œ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤.
    const cleanupResources = useCallback(() => {
        // 1. WebSocket ì—°ê²° ë‹«ê¸°
        if (wsRef.current) {
            wsRef.current.close();
            wsRef.current = null;
            console.log("WebSocket ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¨.");
        }

        // 2. ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ í•´ì œ
        if (mediaStreamRef.current) {
            mediaStreamRef.current.getTracks().forEach(track => {
                track.stop();
            });
            mediaStreamRef.current = null;
            console.log("ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¨.");
        }

        // 3. AudioWorklet/í”„ë¡œì„¸ì„œ ë…¸ë“œ í•´ì œ
        if (audioProcessorRef.current) {
            audioProcessorRef.current.disconnect();
            audioProcessorRef.current = null;
            console.log("ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¨.");
        }

        // AudioContextëŠ” ëª¨ë“  ë…¸ë“œì™€ ìŠ¤íŠ¸ë¦¼ì´ í•´ì œëœ í›„ ë‹«ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
        if (audioContextRef.current) {
            audioContextRef.current.close();
            audioContextRef.current = null;
            console.log("AudioContext ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¨.");
        }
    }, []);

    // -----------------------------------------------------------
    // A. ë§ˆì´í¬ ì ‘ê·¼ ë° ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
    // -----------------------------------------------------------
    const startRecording = useCallback(async () => {
        if (isRecording) return;

        try {
            // 1. ë§ˆì´í¬ ì ‘ê·¼ ë° WebSocket ì—°ê²° (ê¸°ì¡´ê³¼ ë™ì¼)
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: { 
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: AUDIO_CONFIG.sampleRate
                 } 
            });
            mediaStreamRef.current = stream;
            
            const ws = new WebSocket(WS_URL + "?translate=true");
            wsRef.current = ws;

            await new Promise((resolve, reject) => {
                ws.onopen = resolve;
                ws.onerror = reject;
            });
            
            // 2. AudioWorklet í™˜ê²½ ì„¤ì •
            const audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: AUDIO_CONFIG.sampleRate,
            });
            audioContextRef.current = audioContext;
            
            // 3. [í•µì‹¬] AudioWorklet Processor íŒŒì¼ ë¡œë“œ (await í•„ìš”)
            await audioContext.audioWorklet.addModule('/mic-processor.js'); 
            
            // 4. ë…¸ë“œ ìƒì„±: MediaStreamSource -> AudioWorkletNode
            const input = audioContext.createMediaStreamSource(stream);
            
            // AudioWorkletì— ë“±ë¡ëœ ì´ë¦„('mic-processor')ìœ¼ë¡œ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            const processor = new AudioWorkletNode(audioContext, 'mic-processor');
            audioProcessorRef.current = processor;
            
            // 5. [í•µì‹¬] AudioWorkletì—ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ WebSocketìœ¼ë¡œ ì¤‘ê³„í•˜ëŠ” ë¡œì§
            //    AudioWorkletì€ ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ MessagePortë¥¼ í†µí•´ ë³´ëƒ…ë‹ˆë‹¤.
            processor.port.onmessage = (event) => {
                const audioBuffer = event.data; // Workletìœ¼ë¡œë¶€í„° ë°›ì€ Int16Array.buffer
                
                // WebSocketì´ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸ í›„ ì „ì†¡
                if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
                    wsRef.current.send(audioBuffer);
                }
            };

            // 6. ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì—°ê²°: ë§ˆì´í¬ -> í”„ë¡œì„¸ì„œ -> ëª©ì ì§€(ì¶œë ¥)
            // processorë¥¼ destinationì— ì—°ê²°í•´ì•¼ ë¸Œë¼ìš°ì €ê°€ í™œì„± ìƒíƒœë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
            input.connect(processor);
            processor.connect(audioContext.destination);

            setIsRecording(true);
            setTranscript('ğŸ™ï¸ ì‹¤ì‹œê°„ ì „ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (AudioWorklet ë°©ì‹)');

        } catch (e) {
            console.error("ë§ˆì´í¬/WebSocket ì—°ê²° ì˜¤ë¥˜:", e);
            alert("ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆê±°ë‚˜ WebSocket ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.");
            cleanupResources();
            setIsRecording(false);
        }
    }, [isRecording, cleanupResources]);
    

    // -----------------------------------------------------------
    // B. ë§ˆì´í¬ ë° WebSocket ì—°ê²° ì¤‘ì§€
    // -----------------------------------------------------------
    const stopRecording = useCallback(() => {
        if (!isRecording) return;
        
        cleanupResources();

        // 4. ìƒíƒœ ì´ˆê¸°í™”
        setIsRecording(false);
        setPartialText('');
        // ìµœì¢… ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë‚¨ê²¨ë‘ê±°ë‚˜, 'ìµœì¢… ìš”ì•½ ì¤‘...' ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        setTranscript(prev => prev + '\n[ë…¹ìŒ ì¢…ë£Œ]');
        console.log("ë…¹ìŒ ì¤‘ì§€ ì™„ë£Œ.");

    }, [isRecording, cleanupResources]);

    // -----------------------------------------------------------
    // C. WebSocket ë©”ì‹œì§€ ìˆ˜ì‹ 
    // -----------------------------------------------------------
    useEffect(() => {
        const ws = wsRef.current; // í˜„ì¬ WebSocket ì°¸ì¡°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        if (ws) {
            // 1. ì„œë²„ë¡œë¶€í„° ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í–ˆì„ ë•Œ í˜¸ì¶œë  í•¸ë“¤ëŸ¬
            ws.onmessage = (event) => {
                try {
                    const message = JSON.parse(event.data);
                    
                    switch (message.type) {
                        case 'partial_transcript':
                            // ì„ì‹œ ì „ì‚¬ í…ìŠ¤íŠ¸ (Deepgram ì‘ë‹µ)
                            setPartialText(message.text);
                            break;
                            
                        case 'final_transcript':
                            // ìµœì¢… ì „ì‚¬ í…ìŠ¤íŠ¸ (Deepgram ì‘ë‹µ)
                            // ìµœì¢… í…ìŠ¤íŠ¸ëŠ” ëˆ„ì ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ê¸°ì¡´ transcriptì— ì¶”ê°€
                            setTranscript(prev => prev + '\n' + message.text);
                            setPartialText(''); // ì„ì‹œ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                            break;
                            
                        case 'translation':
                            // ë²ˆì—­ ê²°ê³¼ (LLM Service ì‘ë‹µ)
                            setTranslation(message.translated_text);
                            // ì°¸ê³ : ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë²ˆì—­ ê²°ê³¼ë¥¼ ìµœì¢… ì „ì‚¬ ì˜†ì— ë¶™ì´ê±°ë‚˜
                            // ë³„ë„ì˜ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
                            break;
                            
                        case 'error':
                            // ì„œë²„ ì¸¡ ì˜¤ë¥˜ (FastAPI/Deepgram/OpenAI ì˜¤ë¥˜)
                            console.error("Server Error:", message.message);
                            setPartialText(`[ERROR]: ${message.message}`);
                            break;
                            
                        // (ì¶”í›„ summary, action_items ë“±ì˜ typeë„ ì—¬ê¸°ì„œ ì²˜ë¦¬)
                        
                        default:
                            console.warn("Unknown message type received:", message.type);
                    }
                } catch (e) {
                    console.error("WebSocket Message Parsing Error:", e);
                }
            };
            
            // 2. ì—°ê²° ì¢…ë£Œ ì‹œ
            ws.onclose = () => {
                console.log("WebSocket ì—°ê²°ì´ ì„œë²„/í´ë¼ì´ì–¸íŠ¸ ì¸¡ì—ì„œ ë‹«í˜”ìŠµë‹ˆë‹¤.");
                // isRecordingì´ Trueì¸ ìƒíƒœì—ì„œ ì—°ê²°ì´ ëŠê¸°ë©´ ì˜¤ë¥˜ë¡œ ê°„ì£¼í•˜ê³  ìƒíƒœ ì •ë¦¬
                if (isRecording) {
                    stopRecording();
                    setTranscript(prev => prev + '\n[ì„œë²„ ì—°ê²° ì˜¤ë¥˜ë¡œ ì¢…ë£Œ]');
                }
            };
        }
        
        // 3. í´ë¦°ì—…(Cleanup) í•¨ìˆ˜: ì»´í¬ë„ŒíŠ¸ê°€ ì–¸ë§ˆìš´íŠ¸ë  ë•Œ ì‹¤í–‰
        // (ì£¼ì˜) ì´ useEffectëŠ” wsRef.currentê°€ ë°”ë€” ë•Œë§ˆë‹¤ ì‹¤í–‰ë˜ë¯€ë¡œ, 
        // ê¸°ì¡´ì˜ onmessage í•¸ë“¤ëŸ¬ë¥¼ í•´ì œí•˜ëŠ” í´ë¦°ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.
        return () => {
            if (ws) {
                // í•¸ë“¤ëŸ¬ ì¤‘ë³µ ë“±ë¡ ë°©ì§€
                ws.onmessage = null; 
                ws.onclose = null;
            }
        };
    }, [stopRecording, wsRef.current]);


    // 3. ì™¸ë¶€ì— ë…¸ì¶œí•  ìƒíƒœì™€ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    return {
        isRecording,
        transcript,
        partialText,
        translation,
        startRecording,
        stopRecording
    };
};

export default useRealtimeStream;